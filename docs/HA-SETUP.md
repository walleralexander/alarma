# üöÄ High Availability Setup - Optional

**Alarma! Notification Gateway**
**WebPoint Internet Solutions**
**Version:** 1.0 | **Datum:** 30. Januar 2026

---

## üìã √úbersicht

Dieses Dokument beschreibt **konzeptionell**, wie Alarma! f√ºr hochverf√ºgbare (High Availability, HA) Szenarien erweitert werden kann. Die Standard-Installation ist f√ºr kleine bis mittlere Umgebungen ausgelegt. F√ºr gesch√§ftskritische Anwendungen mit strengen Uptime-Anforderungen bietet dieses Konzept Erweiterungsm√∂glichkeiten.

**Wichtig:** Dies ist ein **Konzept-Dokument**, keine Schritt-f√ºr-Schritt-Anleitung. Die Implementierung erfordert fortgeschrittene Kenntnisse in Container-Orchestrierung und Load Balancing.

---

## üéØ Ziele einer HA-Architektur

### Prim√§re Ziele

- **Elimination Single Points of Failure:** Kein Ausfall bei Hardware-/Software-Problemen
- **Automatisches Failover:** Nahtloser Wechsel bei Ausf√§llen
- **Load Distribution:** Lastverteilung √ºber mehrere Nodes
- **Skalierbarkeit:** Horizontale Skalierung bei erh√∂hter Last
- **Geografische Redundanz:** Schutz vor Standort-Ausfall

### Typische Uptime-Ziele

| Availability | Downtime/Jahr | Use Case |
| ------------ | ------------- | ------------- |
| 99% | 3,65 Tage | Standard-Betrieb |
| 99,9% | 8,76 Stunden | Business-kritisch |
| 99,99% | 52,6 Minuten | Mission-kritisch |
| 99,999% | 5,26 Minuten | Carrier-Grade |

**Alarma! Standard:** ~99% (3-4 Tage/Jahr)  
**Alarma! HA-Setup:** ~99,9% (<9 Stunden/Jahr)

---

## üèóÔ∏è HA-Architektur Konzepte

### Konzept 1: Active-Passive (Einfach)

**Beschreibung:** Ein aktiver Node bedient Anfragen, ein passiver steht bereit.

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Load Balancer / VIP               ‚îÇ
‚îÇ         (HAProxy / Keepalived)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                 ‚îÇ
           ‚ñº                 ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Node 1  ‚îÇ      ‚îÇ  Node 2  ‚îÇ
    ‚îÇ (ACTIVE) ‚îÇ      ‚îÇ(PASSIVE) ‚îÇ
    ‚îÇ          ‚îÇ      ‚îÇ          ‚îÇ
    ‚îÇ Alarma! ‚îÇ      ‚îÇ Alarma! ‚îÇ
    ‚îÇ  Stack   ‚îÇ      ‚îÇ  Stack   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                 ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚ñº
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ Shared       ‚îÇ
          ‚îÇ Storage      ‚îÇ
          ‚îÇ (NFS/GlusterFS)‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Vorteile:**

- ‚úÖ Einfache Implementierung
- ‚úÖ Klare Zustandsverwaltung
- ‚úÖ Keine Split-Brain-Problematik

**Nachteile:**

- ‚ùå 50% der Ressourcen ungenutzt
- ‚ùå Manuelles oder automatisiertes Failover n√∂tig
- ‚ùå Keine Last-Verteilung

**Failover-Zeit:** 30-60 Sekunden

### Konzept 2: Active-Active (Load Balanced)

**Beschreibung:** Mehrere Nodes bedienen Anfragen parallel, Load Balancer verteilt.

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Load Balancer                     ‚îÇ
‚îÇ        (Traefik / HAProxy / nginx)          ‚îÇ
‚îÇ      Round-Robin / Least Connections        ‚îÇ
‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ
   ‚ñº      ‚ñº      ‚ñº      ‚ñº      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇN1  ‚îÇ ‚îÇN2  ‚îÇ ‚îÇN3  ‚îÇ ‚îÇN4  ‚îÇ ‚îÇN5  ‚îÇ
‚îÇ(A) ‚îÇ ‚îÇ(A) ‚îÇ ‚îÇ(A) ‚îÇ ‚îÇ(A) ‚îÇ ‚îÇ(A) ‚îÇ
‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îò
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  Shared DB  ‚îÇ
         ‚îÇ  (Redis/    ‚îÇ
         ‚îÇ   Postgres) ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Vorteile:**

- ‚úÖ Volle Ressourcen-Nutzung
- ‚úÖ Automatische Last-Verteilung
- ‚úÖ Horizontal skalierbar
- ‚úÖ Kein Single Point of Failure

**Nachteile:**

- ‚ùå Komplexere Implementierung
- ‚ùå Session/State-Management n√∂tig
- ‚ùå Shared Storage erforderlich

**Failover-Zeit:** < 1 Sekunde (automatisch)

### Konzept 3: Geo-Redundant (Multi-Site)

**Beschreibung:** Nodes an verschiedenen geografischen Standorten.

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       Global Load Balancer (GeoDNS)         ‚îÇ
‚îÇ      (Cloudflare / Route53 / NS1)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                  ‚îÇ
    Standort A          Standort B
           ‚îÇ                  ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Cluster A ‚îÇ    ‚îÇ   Cluster B ‚îÇ
    ‚îÇ   (3 Nodes) ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚î§   (3 Nodes) ‚îÇ
    ‚îÇ             ‚îÇ    ‚îÇ             ‚îÇ
    ‚îÇ  Alarma!   ‚îÇ    ‚îÇ  Alarma!   ‚îÇ
    ‚îÇ   Stack     ‚îÇ    ‚îÇ   Stack     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         Replication / Sync
```

**Vorteile:**

- ‚úÖ Schutz vor Standort-Ausfall
- ‚úÖ Reduzierte Latenz (Geo-Routing)
- ‚úÖ Disaster Recovery integriert

**Nachteile:**

- ‚ùå Sehr komplex
- ‚ùå Hohe Kosten
- ‚ùå Daten-Synchronisation herausfordernd

**Failover-Zeit:** 1-5 Minuten (DNS-Propagation)

---

## üîß Technologie-Stack Optionen

### Load Balancer

#### Option 1: HAProxy

**Pro:**

- Sehr performant (Layer 4 + 7)
- Umfangreiche Health Checks
- Session Persistence
- Weit verbreitet, stabil

**Konfigurationsbeispiel:**

```haproxy
# /etc/haproxy/haproxy.cfg
frontend apprise_frontend
    bind *:8000
    mode http
    default_backend apprise_nodes

backend apprise_nodes
    mode http
    balance roundrobin
    option httpchk GET /
    http-check expect status 200
    
    server node1 192.168.1.10:8000 check
    server node2 192.168.1.11:8000 check
    server node3 192.168.1.12:8000 check backup
```

#### Option 2: Traefik

**Pro:**

- Native Docker/Kubernetes Integration
- Automatische Service Discovery
- Let's Encrypt Integration
- Modern, Container-nativ

**Konfigurationsbeispiel:**

```yaml
# docker-compose.yml
services:
  traefik:
    image: traefik:v2.10
    command:
      - "--providers.docker=true"
      - "--entrypoints.web.address=:8000"
      - "--api.insecure=false"
    ports:
      - "8000:8000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro

  apprise-api:
    deploy:
      replicas: 3
      update_config:
        parallelism: 1
        delay: 10s
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.apprise.rule=Host(`notifications.local`)"
      - "traefik.http.services.apprise.loadbalancer.server.port=8000"
```

#### Option 3: nginx

**Pro:**

- Lightweight
- Sehr weit verbreitet
- Einfache Konfiguration

**Konfigurationsbeispiel:**

```nginx
upstream apprise_backend {
    least_conn;  # Least Connections Algorithm
    
    server 192.168.1.10:8000 max_fails=3 fail_timeout=30s;
    server 192.168.1.11:8000 max_fails=3 fail_timeout=30s;
    server 192.168.1.12:8000 backup;  # Backup Node
}

server {
    listen 8000;
    
    location / {
        proxy_pass http://apprise_backend;
        proxy_next_upstream error timeout http_500 http_502 http_503;
        proxy_connect_timeout 5s;
        
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
}
```

### Container Orchestrierung

#### Option 1: Docker Swarm (Einfach)

**Pro:**

- Native Docker Integration
- Einfache Syntax
- Schnelle Einrichtung
- Ausreichend f√ºr kleine/mittlere Setups

**Beispiel:**

```bash
# Swarm initialisieren
docker swarm init

# Service mit 3 Replicas deployen
docker stack deploy -c docker-compose.yml alarma

# Auto-Scaling
docker service scale alarma_apprise-api=5
```

**docker-compose.yml f√ºr Swarm:**

```yaml
version: '3.8'

services:
  apprise-api:
    image: lscr.io/linuxserver/apprise-api:latest
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
      placement:
        constraints:
          - node.role == worker
    networks:
      - alarma_network

networks:
  alarma_network:
    driver: overlay
```

#### Option 2: Kubernetes (Enterprise)

**Pro:**

- Industry Standard
- Sehr skalierbar
- Umfangreiches √ñkosystem
- Cloud-Provider Support

**Con:**

- Hohe Komplexit√§t
- Steile Lernkurve
- Overhead f√ºr kleine Setups

**Beispiel Deployment:**

```yaml
# apprise-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: apprise-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: apprise-api
  template:
    metadata:
      labels:
        app: apprise-api
    spec:
      containers:
      - name: apprise
        image: lscr.io/linuxserver/apprise-api:latest
        ports:
        - containerPort: 8000
        livenessProbe:
          httpGet:
            path: /
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: apprise-service
spec:
  selector:
    app: apprise-api
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
  type: LoadBalancer
```

### Shared Storage

#### Option 1: NFS (Network File System)

**Pro:**

- Einfach einzurichten
- Weit unterst√ºtzt
- Gut f√ºr Read-Heavy Workloads

**Con:**

- Single Point of Failure (ohne HA-NFS)
- Performance-Limitierungen

#### Option 2: GlusterFS

**Pro:**

- Distributed, repliziert
- Keine Metadaten-Server (kein SPOF)
- Gut f√ºr mittlere Gr√∂√üe

**Con:**

- Komplexer Setup
- Overhead bei kleinen Files

#### Option 3: Ceph

**Pro:**

- Highly scalable
- Object + Block + File Storage
- Enterprise-Grade

**Con:**

- ‚ùå Sehr komplex
- Hoher Ressourcen-Bedarf

---

## üìä Komponenten-spezifische √úberlegungen

### Apprise API (Stateless)

**Herausforderung:** Prinzipiell stateless, aber Config-Files.

**L√∂sung:**

- Configs in Shared Storage oder
- Config-Management (Consul, etcd) oder
- ConfigMaps (Kubernetes)

**Empfehlung:** Shared NFS Volume f√ºr `/config`

### SMS Gateway (Android-gebunden)

**Herausforderung:** Android-Smartphone als physisches Gateway.

**L√∂sungen:**

1. **Mehrere Android-Ger√§te mit Load Balancing:**
   - Gateway-Container auf verschiedenen Nodes
   - Jeder Node hat eigenes Android-Device
   - Load Balancer verteilt SMS-Requests

2. **Android Gateway als Shared Resource:**
   - Ein zentrales Android-Device
   - Mehrere Gateway-Container greifen darauf zu
   - USB-over-IP oder Network-Sharing

**Empfehlung:** Option 1 f√ºr echte HA

### WhatsApp/Signal (Session-basiert)

**Herausforderung:** QR-Code Pairing, Session State.

**L√∂sungen:**

- Session-Daten in Shared Storage
- Session Affinity im Load Balancer (Sticky Sessions)
- Regelm√§√üige Session-Backups

**Empfehlung:** Sticky Sessions + Shared Storage

### ntfy (Publish/Subscribe)

**Herausforderung:** Subscriber-Verbindungen, Message Queue.

**L√∂sung:**

- Zentrale ntfy-Instanz mit Redis Backend
- Oder: ntfy Pro mit HA-Features

---

## ‚öñÔ∏è Trade-offs & Entscheidungshilfe

### Wann HA sinnvoll ist

‚úÖ **JA zu HA wenn:**

- Benachrichtigungen gesch√§ftskritisch (z.B. Feuerwehr, Krankenhaus)
- SLA-Anforderungen > 99%
- Budget f√ºr zus√§tzliche Hardware/Maintenance
- Team-Expertise vorhanden
- Wartungsfenster nicht akzeptabel

‚ùå **NEIN zu HA wenn:**

- Standard IT-Umgebung (Office-Stunden)
- Backup-Kommunikationskan√§le existieren (Telefon)
- Wartungsfenster machbar (nachts/Wochenende)
- Budget/Ressourcen limitiert
- Einfachheit wichtiger als Uptime

### Kosten-Nutzen-Analyse

**Standard Setup (Single Node):**

- Hardware: 1 Server
- Kosten: ~‚Ç¨250 + Betriebskosten
- Uptime: ~99%

**Active-Passive HA:**

- Hardware: 2 Server + Shared Storage
- Kosten: ~‚Ç¨1.500-2.000
- Uptime: ~99,5%
- Zus√§tzlicher Aufwand: +20h Setup, +5h/Monat Wartung

**Active-Active HA (3-Node Cluster):**

- Hardware: 3 Server + Load Balancer + Shared Storage
- Kosten: ~‚Ç¨3.000-4.000
- Uptime: ~99,9%
- Zus√§tzlicher Aufwand: +40h Setup, +10h/Monat Wartung

---

## üõ†Ô∏è Implementierungs-Roadmap (Konzeptionell)

### Phase 1: Single-Node Optimierung (2-4 Wochen)

- [ ] Container Health Checks implementieren
- [ ] Restart Policies optimieren
- [ ] Monitoring aufsetzen (Prometheus/Grafana)
- [ ] Automatische Backups einrichten
- [ ] Dokumentierte Restore-Prozeduren

**Uptime-Verbesserung:** 95% ‚Üí 99%

### Phase 2: Active-Passive Setup (4-8 Wochen)

- [ ] Zweiten Node aufsetzen
- [ ] Shared Storage (NFS) einrichten
- [ ] Keepalived f√ºr VIP-Failover
- [ ] Automated Health Checks & Failover
- [ ] Monitoring erweitern (beide Nodes)
- [ ] Failover-Tests durchf√ºhren

**Uptime-Verbesserung:** 99% ‚Üí 99,5%

### Phase 3: Load-Balanced Active-Active (8-12 Wochen)

- [ ] Dritten Node hinzuf√ºgen
- [ ] HAProxy/Traefik einrichten
- [ ] Session Affinity konfigurieren
- [ ] Auto-Scaling implementieren
- [ ] Performance-Tests
- [ ] Disaster Recovery Tests

**Uptime-Verbesserung:** 99,5% ‚Üí 99,9%

### Phase 4: Geo-Redundanz (Optional, 12-20 Wochen)

- [ ] Zweiten Standort evaluieren
- [ ] Data Replication Setup
- [ ] GeoDNS konfigurieren
- [ ] Cross-Site Failover testen
- [ ] Disaster Recovery Drills

**Uptime-Verbesserung:** 99,9% ‚Üí 99,95%+

---

## ‚úÖ Checkliste: HA-Readiness

### Infrastruktur

- [ ] Mindestens 2 physische Server verf√ºgbar
- [ ] Netzwerk mit ausreichend Bandbreite (1 Gbit+)
- [ ] Shared Storage L√∂sung vorhanden/geplant
- [ ] Load Balancer Hardware/Software verf√ºgbar
- [ ] Redundante Stromversorgung (USV)
- [ ] Redundante Netzwerk-Verbindungen

### Team & Prozesse

- [ ] Team mit Docker/Orchestrierung Erfahrung
- [ ] 24/7 On-Call Bereitschaft (oder B√ºrozeiten)
- [ ] Monitoring & Alerting Setup
- [ ] Dokumentierte Runbooks
- [ ] Regelm√§√üige Failover-Drills geplant

### Technisch

- [ ] Apprise Config externalisiert
- [ ] Secrets Management implementiert
- [ ] Health Checks f√ºr alle Services
- [ ] Automated Backups & Restore getestet
- [ ] Logging zentral aggregiert

### Business

- [ ] Budget f√ºr zus√§tzliche Hardware genehmigt
- [ ] SLA-Anforderungen dokumentiert
- [ ] Stakeholder √ºber Maintenance informiert
- [ ] TCO (Total Cost of Ownership) kalkuliert

---

## üÜò Support & Weiterf√ºhrende Ressourcen

### Dokumentation

- **Docker Swarm:** <https://docs.docker.com/engine/swarm/>
- **Kubernetes:** <https://kubernetes.io/docs/>
- **HAProxy:** <http://www.haproxy.org/>
- **Traefik:** <https://doc.traefik.io/traefik/>

### Best Practices

- **12 Factor App:** <https://12factor.net/>
- **SRE Book (Google):** <https://sre.google/books/>
- **High Availability Patterns:** <https://martinfowler.com/articles/patterns-of-distributed-systems/>

### Kontakt

Bei Fragen zur HA-Implementierung:

**E-Mail:** <office@webpoint.at>  
**Abteilung:** Organisation, Prozessmanagement und IT  
**WebPoint Internet Solutions**

---

## üìå Zusammenfassung

### Key Takeaways

1. **HA ist optional** - Standard-Setup f√ºr die meisten Szenarien ausreichend
2. **Start Simple** - Erst Single-Node optimieren, dann erweitern
3. **Trade-offs verstehen** - Komplexit√§t vs. Uptime vs. Kosten
4. **Testen, testen, testen** - Failover muss geprobt werden
5. **Monitoring ist Pflicht** - Ohne Monitoring keine HA

### Empfehlung f√ºr WebPoint Internet Solutions

**Phase 1 (Sofort):**

- Single-Node Setup optimieren
- Monitoring & Alerting
- Automatische Backups

**Phase 2 (Bei Bedarf):**

- Active-Passive mit 2 Nodes
- Nur wenn SLA-Anforderungen steigen

**Phase 3 (Optional):**

- Active-Active erst bei nachgewiesenem Bedarf

---

**Version:** 1.0  
**Letzte Aktualisierung:** 30. Januar 2026  
**Status:** Konzept-Dokument
